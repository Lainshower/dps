{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "\n",
    "# 필요한 경로 추가\n",
    "base_path = '/home/seonghee_hong/legal_llm/dps'\n",
    "sys.path.append(f'{base_path}/dps/spark')\n",
    "sys.path.append(f'{base_path}/dps/spark/jobs')\n",
    "\n",
    "from jobs.korean_job import *\n",
    "from jobs.dedup_job import *\n",
    "from prep.dedup_prep import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CASE data after first deduplication: 192730\n",
      "Total NON-CASE data after first deduplication: 231179\n"
     ]
    }
   ],
   "source": [
    "### Load Sample Data\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Fix Random Seed\n",
    "random.seed(42)\n",
    "\n",
    "with open('/data/llmlaw/FIRST_DEDUPLICATION/Domain/CASE-LAW/deduplication.jsonl', 'r') as f:\n",
    "    case_data_og = [json.loads(line) for line in f]\n",
    "\n",
    "with open('/data/llmlaw/FIRST_DEDUPLICATION/Domain/NON-CASE-LAW/deduplication.jsonl', 'r') as f:\n",
    "    non_case_data_og = [json.loads(line) for line in f]\n",
    "\n",
    "print(\"Total CASE data after first deduplication:\", len(case_data_og))\n",
    "print(\"Total NON-CASE data after first deduplication:\", len(non_case_data_og))\n",
    "\n",
    "# Random Sample\n",
    "case_sample = random.sample(case_data_og, 20000)\n",
    "non_case_sample = random.sample(non_case_data_og, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text만 추출\n",
    "case_text = [case['text'] for case in case_sample]\n",
    "non_case_text = [non_case['text'] for non_case in non_case_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After make_compat: 이것은 테스트용 샘플 텍스트입니다. 1234와 같은 숫자 혹은 특수기호 *!@# 등이 포함될 수 있습니다.\n",
      "Bad words filter result: True\n",
      "Document length filter result: True\n",
      "Mean word length filter result: True\n",
      "Symbol to word ratio filter result: True\n",
      "Bullet and ellipsis filter result: True\n",
      "Korean word ratio filter result: True\n",
      "Preprocessed text: 이것은 테스트용 샘플 텍스트입니다. 1234와 같은 숫자 혹은 특수기호 *!@# 등이 포함될 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 테스트를 위한 샘플 텍스트\n",
    "sample_text = \"이것은 테스트용 샘플 텍스트입니다. 1234와 같은 숫자 혹은 특수기호 *!@# 등이 포함될 수 있습니다.\"\n",
    "\n",
    "# make_compat() 함수 테스트\n",
    "compat_text = make_compat(sample_text)\n",
    "print(\"After make_compat:\", compat_text)\n",
    "\n",
    "# bad_words_filter() 함수 테스트\n",
    "bad_words_result = bad_words_filter(compat_text)\n",
    "print(\"Bad words filter result:\", bad_words_result)\n",
    "\n",
    "# doc_len_filter() 함수 테스트\n",
    "doc_len_result = doc_len_filter(compat_text, 10, 500)  # 예시로 최소 길이 10, 최대 길이 500 설정\n",
    "print(\"Document length filter result:\", doc_len_result)\n",
    "\n",
    "# mean_word_len_filter() 함수 테스트\n",
    "mean_word_len_result = mean_word_len_filter(compat_text, 2, 10)  # 예시로 최소 평균 단어 길이 2, 최대 10 설정\n",
    "print(\"Mean word length filter result:\", mean_word_len_result)\n",
    "\n",
    "# symbol_to_word_ratio_filter() 함수 테스트\n",
    "symbol_to_word_ratio_result = symbol_to_word_ratio_filter(compat_text, 0.1)  # 예시로 비율 0.1 설정\n",
    "print(\"Symbol to word ratio filter result:\", symbol_to_word_ratio_result)\n",
    "\n",
    "# bullet_ellipsis_filter() 함수 테스트\n",
    "bullet_ellipsis_result = bullet_ellipsis_filter(compat_text, 0.05, 0.05)  # 예시로 비율 각각 0.05 설정\n",
    "print(\"Bullet and ellipsis filter result:\", bullet_ellipsis_result)\n",
    "\n",
    "# korean_word_ratio_filter() 함수 테스트\n",
    "korean_word_ratio_result = korean_word_ratio_filter(compat_text, 0.5)  # 예시로 한국어 비율 0.5 설정\n",
    "print(\"Korean word ratio filter result:\", korean_word_ratio_result)\n",
    "\n",
    "# preprocess_text() 함수 테스트\n",
    "preprocessed_text = preprocess_text(compat_text)\n",
    "print(\"Preprocessed text:\", preprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DPS 상에서 코드 전처리 파이프라인과 동일하게 Chaining 방식으로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "config_path = '/home/seonghee_hong/legal_llm/dps/configs/dedup_job_case_law.yaml'\n",
    "\n",
    "with open(config_path) as f:\n",
    "    conf = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "if conf['targets'] == ['all']:\n",
    "    input_paths = f'{conf[\"base_dir\"]}/*/*.jsonl'\n",
    "else:\n",
    "    # input_paths = ','.join([f'{conf[\"base_dir\"]}/{t}/*.jsonl' for t in conf[\"targets\"]])\n",
    "    input_paths = '/data/llmlaw/FIRST_DEDUPLICATION/Domain/CASE-LAW/sampled.jsonl'\n",
    "\n",
    "# Spark 세션 구성 변경\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"korean text processing job\") \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "    .config(\"spark.eventLog.dir\", \"/home/seonghee_hong/spark-log\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "proc_rdd = sc.textFile(input_paths).repartition(conf[\"n_dist\"]).flatMap(read_line)\n",
    "\n",
    "make_compat_rdd = proc_rdd.map(lambda x: dict(text=make_compat(x[\"text\"])))\n",
    "print(f\"Count after make_compat: {make_compat_rdd.count()}\")\n",
    "\n",
    "bad_words_filter_rdd = make_compat_rdd.filter(lambda x: bad_words_filter(x[\"text\"]))\n",
    "print(f\"Count after bad_words_filter: {bad_words_filter_rdd.count()}\")\n",
    "\n",
    "doc_len_filter_rdd = bad_words_filter_rdd.filter(lambda x: doc_len_filter(x[\"text\"], conf[\"min_doc_len\"], conf[\"max_doc_len\"]))\n",
    "print(f\"Count after doc_len_filter: {doc_len_filter_rdd.count()}\")\n",
    "\n",
    "mean_word_len_filter_rdd = doc_len_filter_rdd.filter(lambda x: mean_word_len_filter(x[\"text\"], conf[\"min_mean_word_len\"], conf[\"max_mean_word_len\"]))\n",
    "print(f\"Count after mean_word_len_filter: {mean_word_len_filter_rdd.count()}\")\n",
    "\n",
    "symbol_to_word_ratio_filter_rdd = mean_word_len_filter_rdd.filter(lambda x: symbol_to_word_ratio_filter(x[\"text\"], conf[\"symbol_to_word_ratio\"]))\n",
    "print(f\"Count after symbol_to_word_ratio_filter: {symbol_to_word_ratio_filter_rdd.count()}\")\n",
    "\n",
    "bullet_ellipsis_filter_rdd = symbol_to_word_ratio_filter_rdd.filter(lambda x: bullet_ellipsis_filter(x[\"text\"], conf[\"bullet_point_ratio\"], conf[\"ellipsis_ratio\"]))\n",
    "print(f\"Count after bullet_ellipsis_filter: {bullet_ellipsis_filter_rdd.count()}\")\n",
    "\n",
    "korean_word_ratio_filter_rdd = bullet_ellipsis_filter_rdd.filter(lambda x: korean_word_ratio_filter(x[\"text\"], conf[\"korean_word_ratio\"]))\n",
    "print(f\"Count after korean_word_ratio_filter: {korean_word_ratio_filter_rdd.count()}\")\n",
    "\n",
    "final_rdd = korean_word_ratio_filter_rdd.map(lambda x: dict(text=preprocess_text(x[\"text\"]))).filter(lambda x: doc_len_filter(x[\"text\"], conf[\"min_doc_len\"], conf[\"max_doc_len\"]))\n",
    "print(f\"Final count after all filters and preprocessing: {final_rdd.count()}\")\n",
    "\n",
    "# Spark 세션 종료\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dps)",
   "language": "python",
   "name": "dps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
