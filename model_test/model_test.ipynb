{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaoara/anaconda3/envs/test/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-21 14:52:12,878\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "\n",
    "\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextDataset, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 14:52:16,539\tINFO worker.py:1781 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-21 14:52:18 config.py:623] Defaulting to use mp for distributed inference\n",
      "INFO 08-21 14:52:18 llm_engine.py:161] Initializing an LLM engine (v0.5.0) with config: model='beomi/Llama-3-KoEn-8B', speculative_config=None, tokenizer='beomi/Llama-3-KoEn-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=beomi/Llama-3-KoEn-8B)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225760)\u001b[0;0m INFO 08-21 14:52:21 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225759)\u001b[0;0m INFO 08-21 14:52:21 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225758)\u001b[0;0m INFO 08-21 14:52:21 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 08-21 14:52:21 utils.py:623] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225759)\u001b[0;0m INFO 08-21 14:52:21 utils.py:623] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225758)\u001b[0;0m INFO 08-21 14:52:21 utils.py:623] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225759)\u001b[0;0m INFO 08-21 14:52:21 pynccl.py:65] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225758)\u001b[0;0m INFO 08-21 14:52:21 pynccl.py:65] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225760)\u001b[0;0m INFO 08-21 14:52:21 utils.py:623] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225760)\u001b[0;0m INFO 08-21 14:52:21 pynccl.py:65] vLLM is using nccl==2.20.5\n",
      "INFO 08-21 14:52:21 pynccl.py:65] vLLM is using nccl==2.20.5\n",
      "WARNING 08-21 14:52:22 custom_all_reduce.py:170] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225760)\u001b[0;0m WARNING 08-21 14:52:22 custom_all_reduce.py:170] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225758)\u001b[0;0m WARNING 08-21 14:52:22 custom_all_reduce.py:170] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225759)\u001b[0;0m WARNING 08-21 14:52:22 custom_all_reduce.py:170] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/kaoara/anaconda3/envs/test/lib/python3.9/multiprocessing/resource_tracker.py\", line 201, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_3abea110'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kaoara/anaconda3/envs/test/lib/python3.9/multiprocessing/resource_tracker.py\", line 201, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_3abea110'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kaoara/anaconda3/envs/test/lib/python3.9/multiprocessing/resource_tracker.py\", line 201, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_3abea110'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-21 14:52:22 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225760)\u001b[0;0m INFO 08-21 14:52:22 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225758)\u001b[0;0m INFO 08-21 14:52:22 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225759)\u001b[0;0m INFO 08-21 14:52:22 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "INFO 08-21 14:52:24 model_runner.py:159] Loading model weights took 3.7417 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225760)\u001b[0;0m INFO 08-21 14:52:24 model_runner.py:159] Loading model weights took 3.7417 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225758)\u001b[0;0m INFO 08-21 14:52:25 model_runner.py:159] Loading model weights took 3.7417 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225759)\u001b[0;0m INFO 08-21 14:52:25 model_runner.py:159] Loading model weights took 3.7417 GB\n",
      "INFO 08-21 14:52:26 distributed_gpu_executor.py:56] # GPU blocks: 70556, # CPU blocks: 8192\n",
      "INFO 08-21 14:52:30 model_runner.py:878] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-21 14:52:30 model_runner.py:882] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225760)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3225759)\u001b[0;0m INFO 08-21 14:52:30 model_runner.py:878] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-21 14:52:30 model_runner.py:878] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225759)\u001b[0;0m INFO 08-21 14:52:30 model_runner.py:882] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225760)\u001b[0;0m INFO 08-21 14:52:30 model_runner.py:882] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225758)\u001b[0;0m INFO 08-21 14:52:30 model_runner.py:878] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225758)\u001b[0;0m INFO 08-21 14:52:30 model_runner.py:882] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225760)\u001b[0;0m INFO 08-21 14:52:37 model_runner.py:954] Graph capturing finished in 7 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225759)\u001b[0;0m INFO 08-21 14:52:37 model_runner.py:954] Graph capturing finished in 7 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3225758)\u001b[0;0m INFO 08-21 14:52:37 model_runner.py:954] Graph capturing finished in 7 secs.\n",
      "INFO 08-21 14:52:37 model_runner.py:954] Graph capturing finished in 7 secs.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "\n",
    "checkpoint = 'beomi/Llama-3-KoEn-8B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir=\"/data/llmlaw/base_model\")\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=5000)\n",
    "vllm_model = LLM(model='beomi/Llama-3-KoEn-8B', tensor_parallel_size=4)\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, cache_dir=\"/data/llmlaw/base_model\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map='', cache_dir=\"/data/llmlaw/base_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_path = '/home/kaoara/dps/legal-project-data/DOMAIN_Preprocessed_CASE_LAW/sampled_100.jsonl'\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data, tokenize, and split into input and label\n",
    "batch_size = 2  # Adjust the batch size based on your GPU memory\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def process_batch(batch):\n",
    "    vllm_input_texts = [item['text'][:300] for item in batch]\n",
    "\n",
    "    label_texts = [item['text'][300:] if len(tokenizer.tokenize(item['text'][300:])) <= 5000 \n",
    "                   else tokenizer.decode(tokenizer.encode(item['text'][300:], truncation=True, max_length=5000), skip_special_tokens=True)\n",
    "                   for item in batch]\n",
    "    \n",
    "    # input_texts = [item['text'] if len(tokenizer.tokenize(item['text'])) <= 5100 \n",
    "    #                else tokenizer.decode(tokenizer.encode(item['text'])[:5100], skip_special_tokens=True)\n",
    "    #                for item in batch]\n",
    "    \n",
    "    # input_ids = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "    \n",
    "    # input_ids = input_ids.to(model.device)\n",
    "\n",
    "    # Generate output using the model\n",
    "    generated_outputs = vllm_model.generate(vllm_input_texts, sampling_params)\n",
    "\n",
    "    # Decode generated text\n",
    "    generated_texts = [output.outputs[0].text for output in generated_outputs]\n",
    "\n",
    "    # Calculate perplexity (PPL)\n",
    "    # with torch.no_grad():\n",
    "    #     outputs = model(input_ids=input_ids)\n",
    "    #     log_likelihood = outputs.loss\n",
    "    #     ppl = torch.exp(log_likelihood)\n",
    "\n",
    "    for i, item in enumerate(batch):\n",
    "        results.append({\n",
    "            'text': item['text'],\n",
    "            'tokenized_output': tokenizer.tokenize(item['text']),\n",
    "            'label': label_texts[i],\n",
    "            'input': item['text'][:300],\n",
    "            'generated_output': generated_texts[i],\n",
    "            # 'ppl': ppl[i].item()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:48<00:00, 24.43s/it, Generation Speed: 113.92 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:50<00:00, 25.36s/it, Generation Speed: 139.72 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:09<00:00,  5.00s/it, Generation Speed: 133.25 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:54<00:00, 27.31s/it, Generation Speed: 154.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:24<00:00, 12.15s/it, Generation Speed: 101.25 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:06<00:00, 33.41s/it, Generation Speed: 149.64 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:12<00:00,  6.36s/it, Generation Speed: 110.53 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:14<00:00, 37.08s/it, Generation Speed: 70.59 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:05<00:00,  2.99s/it, Generation Speed: 123.39 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:57<00:00, 28.66s/it, Generation Speed: 90.45 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:12<00:00, 36.21s/it, Generation Speed: 74.08 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:01<00:00, 30.53s/it, Generation Speed: 93.21 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:08<00:00, 34.35s/it, Generation Speed: 133.60 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:16<00:00, 38.16s/it, Generation Speed: 67.57 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:15<00:00, 37.73s/it, Generation Speed: 68.21 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:13<00:00, 36.88s/it, Generation Speed: 84.80 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:13<00:00,  6.92s/it, Generation Speed: 99.54 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:13<00:00, 36.55s/it, Generation Speed: 91.21 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:55<00:00, 27.67s/it, Generation Speed: 136.68 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:11<00:00, 35.81s/it, Generation Speed: 71.40 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:02<00:00, 31.11s/it, Generation Speed: 83.99 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:44<00:00, 52.07s/it, Generation Speed: 51.36 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:16<00:00, 38.26s/it, Generation Speed: 113.34 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:22<00:00, 11.27s/it, Generation Speed: 140.30 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:22<00:00, 11.35s/it, Generation Speed: 70.71 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:08<00:00, 34.42s/it, Generation Speed: 134.74 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:21<00:00, 10.99s/it, Generation Speed: 98.33 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:33<00:00, 16.79s/it, Generation Speed: 85.13 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:12<00:00, 36.10s/it, Generation Speed: 71.76 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:07<00:00, 33.83s/it, Generation Speed: 135.89 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:14<00:00,  7.02s/it, Generation Speed: 137.54 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:14<00:00, 37.28s/it, Generation Speed: 67.55 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:10<00:00,  5.30s/it, Generation Speed: 114.85 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:13<00:00, 36.60s/it, Generation Speed: 81.83 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:06<00:00, 33.44s/it, Generation Speed: 91.65 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:24<00:00, 12.28s/it, Generation Speed: 97.79 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:14<00:00, 37.18s/it, Generation Speed: 73.17 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:37<00:00, 18.73s/it, Generation Speed: 87.41 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:19<00:00,  9.72s/it, Generation Speed: 105.56 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:10<00:00, 35.25s/it, Generation Speed: 106.76 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:19<00:00, 39.90s/it, Generation Speed: 78.89 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:19<00:00,  9.97s/it, Generation Speed: 81.99 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:15<00:00, 37.58s/it, Generation Speed: 78.53 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:11<00:00, 35.94s/it, Generation Speed: 90.44 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:29<00:00, 14.82s/it, Generation Speed: 83.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:09<00:00,  4.85s/it, Generation Speed: 70.80 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:27<00:00, 43.65s/it, Generation Speed: 78.79 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:15<00:00, 37.55s/it, Generation Speed: 133.17 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:29<00:00, 44.79s/it, Generation Speed: 69.04 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [01:15<00:00, 37.88s/it, Generation Speed: 67.30 toks/s]\n",
      "100%|██████████| 50/50 [44:40<00:00, 53.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# Batch processing\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    batch = data[i:i+batch_size]\n",
    "    process_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'판시사항 \\n 소재지관서의 증명이 없는 농지매매계약의 채권적 효력\\n주문 \\n 원판결을 파기한다.\\n사건을 대전지방법원 합의부에 환송한다.\\n이유 \\n 그러면 원심은 원고가 원피고 사이의 본건 농지매매의 채권계약이 피고의 책임에 돌아갈 사유로 이행불능에 빠지게 된 것이니 이행에 갈음할 손해배상을 청구함이 원고의 소송취지에 의하여 명백한 이상 원심은 마땅히 농지매매의 채권계약이 성립된 것이고 비록 소재지 관서의 농지증명이 없을 지라도 과연 원고주장과 같이 매도인인 피고의 책임에 돌아갈 사유로 채권계약을 이행할 수 없게 된 것인가를 심구하여 판단'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-1]['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'하였어야 할 것인데도 불구하고 그 농지증명이 없음을 이유로 농지매매의 채권계약이 성립되지 않았다고 판단하였음은 부당하다 할 것이니 이점을 지적하는 논지는 이유있다.\\n그러므로 원판결을 파기하고 사건을 원심법원에 환송하기로 하여 관여법관의 일치된 의견으로 주문과 같이 판결한다.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-1]['generated_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'하여야 함에도 불구하고 만연히 소재지관서의 농지증명이 없는 이상 매매계약은 아직 효력이 발생 할 수 없다 하여 원고의 청구를 쉽사리 배척하였음은 농지 매매에 있어서의 소재지관서의 증명을 요하는 의 법의를 잘못 이해함으로써 심리를 다하지 않은 위법이 있다 할 것이며 원판결에 위에 말한 위법이 있다는 취지의 피고 소송대리인의 상고논지는 이유있다.\\n그러므로 원판결을 파기하고 원심으로 하여금 다시 심리 재판하게 하기 위하여 관여한 법관 전원의 일치된 의견으로 주문과 같이 판결한다.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-1]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a JSONL file\n",
    "output_path = f\"{checkpoint.split('/')[-1]}_results.jsonl\"\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for result in results:\n",
    "        f.write(json.dumps(result, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaoara/anaconda3/envs/test/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-22 07:48:53,362\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "\n",
    "\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextDataset, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:10<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "\n",
    "checkpoint = 'beomi/Llama-3-KoEn-8B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir=\"/data/llmlaw/base_model\")\n",
    "# sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=5000)\n",
    "# vllm_model = LLM(model='beomi/Llama-3-KoEn-8B', tensor_parallel_size=4)\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, cache_dir=\"/data/llmlaw/base_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map='auto', cache_dir=\"/data/llmlaw/base_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_path = '/home/kaoara/dps/legal-project-data/DOMAIN_Preprocessed_CASE_LAW/sampled_100.jsonl'\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data, tokenize, and split into input and label\n",
    "batch_size = 4  # Adjust the batch size based on your GPU memory\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def process_batch(batch):\n",
    "    input_texts = [\n",
    "        item['text'] if len(tokenizer.tokenize(item['text'])) <= 5000  # Redundant\n",
    "        else tokenizer.decode(tokenizer.encode(item['text'], truncation=True, max_length=5100), skip_special_tokens=True)\n",
    "        for item in batch\n",
    "    ]\n",
    "\n",
    "    # Tokenize inputs with padding and truncation\n",
    "    input_ids = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "    input_ids = input_ids.to(model.device)\n",
    "\n",
    "    # Calculate loss for each sample individually\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Initialize a list to store PPL for each sample\n",
    "        ppl_list = []\n",
    "\n",
    "        # Iterate through each sample in the batch\n",
    "        for i in range(batch_size):\n",
    "            # Compute loss for the individual sample\n",
    "            shift_logits = logits[i, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[i, 1:].contiguous()\n",
    "\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction='sum', ignore_index=tokenizer.pad_token_id)\n",
    "            individual_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "            # Calculate PPL for the individual sample\n",
    "            non_pad_tokens = (shift_labels != tokenizer.pad_token_id).sum()\n",
    "            ppl = torch.exp(individual_loss / non_pad_tokens)\n",
    "            ppl_list.append(ppl.item())\n",
    "\n",
    "    # Record results\n",
    "    for i, item in enumerate(batch):\n",
    "        results.append({\n",
    "            'text': item['text'],\n",
    "            'input': input_texts[i],\n",
    "            'ppl': ppl_list[i]\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 25/25 [03:24<00:00,  8.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# Batch processing\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    batch = data[i:i+batch_size]\n",
    "    process_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a JSONL file\n",
    "output_path = f\"{checkpoint.split('/')[-1]}_ppl.jsonl\"\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for result in results:\n",
    "        f.write(json.dumps(result, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PPL: 3.0854262924194336\n"
     ]
    }
   ],
   "source": [
    "def calculate_average_ppl(results):\n",
    "    if not results:\n",
    "        return 0.0\n",
    "    total_ppl = sum(item['ppl'] for item in results)\n",
    "    average_ppl = total_ppl / len(results)\n",
    "    return average_ppl\n",
    "\n",
    "# Example usage\n",
    "# Assuming you've already processed batches and populated the `results` list\n",
    "average_ppl = calculate_average_ppl(results)\n",
    "print(f\"Average PPL: {average_ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dedup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
